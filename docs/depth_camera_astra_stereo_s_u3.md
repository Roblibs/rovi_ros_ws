# Depth camera — Orbbec Astra Stereo S U3 (Yahboom “AI View Depth Camera”)

This device presents as **two separate Linux devices** over one cable:

- **Depth/IR** via **OpenNI2** (Orbbec) → ROS driver: `openni2_camera`
- **RGB** via **UVC / V4L2** → ROS driver: `v4l2_camera`

ROVI’s current policy is to keep depth and RGB as **separate feeds** (no registered RGB-D / pointcloud yet) and provide a stable TF mount and consistent ROS topic names for RViz and downstream stacks.

## Status

- Streaming works: depth + RGB images show in RViz via `stack:=camera`.
- Depth modes for this camera are `640x400` / `320x200` (not `640x480` / `320x240`).
- RGB calibration is required for correct `CameraInfo` (see “Calibration”).

## Run

On the robot:
```bash
camera
```

Equivalent:
```bash
ros2 launch rovi_bringup rovi.launch.py robot_mode:=real stack:=camera rviz:=false
```

On a PC to visualize:
```bash
view camera
```

## ROS contract (topics + TF)

Depth (OpenNI2) under `/camera/depth`:
- `/camera/depth/image_raw` (`16UC1`, mm)
- `/camera/depth/image` (`32FC1`, m)
- `/camera/depth/camera_info`

Color (UVC) under `/camera/color`:
- `/camera/color/image_raw`
- `/camera/color/camera_info`

TF is published by `robot_state_publisher` from `src/rovi_description/urdf/rovi.urdf`:
- `base_link -> camera_link` (mount)
- `camera_link -> camera_depth_frame` (approx internal offset)
- `camera_depth_frame -> camera_depth_optical_frame`
- `camera_link -> camera_color_frame` (approx internal offset)
- `camera_color_frame -> camera_color_optical_frame`

If you move the camera mount, update the fixed joint in `src/rovi_description/urdf/rovi.urdf` (`camera_joint` origin).

## Device selection

### RGB (recommended: stable `/dev/v4l/by-id/...`)

Avoid `/dev/video0` because it can renumber when other cameras are added.

Override explicitly if needed:
```bash
ros2 launch rovi_bringup rovi.launch.py stack:=camera \
  rgb_video_device:=/dev/v4l/by-id/<your-camera>-video-index0
```

### Depth (OpenNI2 `device_id`)

`device_id:="#1"` means “first OpenNI2 enumerated device”. If you ever plug multiple OpenNI2 devices, set `device_id` using:
```bash
ros2 run openni2_camera list_devices
```

## OpenNI2 mode note (`640x400`)

The upstream `openni2_camera` selects modes by named presets (e.g. VGA → `640x480`).
This workspace carries a small `openni2_camera` overlay patch in `src/openni2_camera` so this camera’s `640x400` / `320x200` modes are selectable by name, defaulting to `ORBBEC_640x400_30Hz`.

If you see errors about unsupported `640x480` modes and the images are black, re-check that:

- you rebuilt the workspace after pulling changes (`build`)
- you’re running with the Orbbec-friendly mode names (directly or via `stack:=camera`)

## Calibration

This section is intentionally pragmatic: today, depth and color are produced by two different drivers, but we still want a single, predictable “camera contract” (topics + TF + calibration files) so downstream consumers don’t care.

### Phase 1 — what to store (where the truth lives)

**A) Per-stream intrinsics + distortion (`sensor_msgs/CameraInfo`)**

- Store intrinsics for each stream as a `camera_info_manager` YAML (ROS standard).
- Recommended locations:
  - Per-robot: `~/.ros/camera_info/` (most common; generated by ROS tools)
  - Optional repo-tracked defaults: only for placeholders/templates (avoid committing real per-unit calibration).

**B) Extrinsics between frames (TF)**

- Store robot mount (`base_link -> camera_link`) as fixed URDF joints (`src/rovi_description/urdf/rovi.urdf`).
- Store the *internal* RGB↔depth offset as TF too (either measured and set in the URDF, or later computed and then encoded back into the URDF).
- Today’s `camera_link -> camera_depth_frame` / `camera_link -> camera_color_frame` offsets are placeholders; treat them as “good enough to visualize” until measured.

### Phase 2 — who generates it (how we get calibration files/TF)

**RGB intrinsics**

Calibrate the RGB stream and provide real `camera_info`.

UVC cameras typically do not provide usable intrinsics automatically. Calibrate the RGB stream and provide real `camera_info`.

Recommended tool:
```bash
ros2 run camera_calibration cameracalibrator \
  --size 8x6 --square 0.025 \
  image:=/camera/color/image_raw camera:=/camera/color
```

Notes:
- `--size` is inner corners; `--square` is meters.
- This typically uses `/camera/color/set_camera_info` to store calibration (commonly under `~/.ros/camera_info/`).

**Depth intrinsics**

- In many OpenNI2/Orbbec devices, depth intrinsics come from the device (factory params). For now, we treat depth as “factory-calibrated enough” for visualization and for future depth-only processing (free-space / obstacle clearance in the depth camera frame).
- If we later need higher precision, we can revisit depth intrinsics, but it’s usually lower ROI than getting RGB intrinsics right.

**RGB↔depth relative pose (if/when we need it)**

We do **not** need RGB↔depth registration for the current stack, but if we later want consistent overlay / projection, there are two integration-friendly paths:

1) **Reuse factory extrinsics via OpenNI2 (Yahboom-style)**  
   Yahboom’s `ros2_astra_camera` reads Orbbec/OpenNI2 “camera params” from the device and uses them to populate default intrinsics (and some stereo/extrinsic-related fields), while still allowing overrides via `color_info_url` / `ir_info_url`. This suggests a future “tiny helper” approach for ROVI:
   - query the device’s factory RGB/IR params via OpenNI2 extension APIs
   - publish a measured/static TF between `camera_depth_frame` and `camera_color_frame`
   - keep driver binaries out of the repo (still rely on system-installed OpenNI2 runtime)

2) **Do an explicit calibration run (offline/ROS tool) and write TF**  
   Use a calibration target visible in both streams, estimate the transform, then encode it as the static TF used by launch. (Tooling choice can be ROS-based or offline; we’ll pick based on reliability and friction on Jazzy.)

### What Yahboom does (flat comparison, for calibration only)

In Yahboom’s integrated node (`ros2_astra_camera`):
- It optionally loads calibration YAML via `camera_info_manager` using `color_info_url` and `ir_info_url`.
- If YAML is not provided, it tries to read “factory” camera parameters from the device via OpenNI2/Orbbec extension properties and synthesizes reasonable `CameraInfo` defaults.
- This is a useful reference model for us (separating “factory defaults” vs “explicit calibration overrides”) even if we keep the split-driver architecture.

## Simulation (`sim camera`)

Simulation publishes camera topics from Gazebo sensors defined in `src/rovi_description/urdf/rovi.urdf` and bridges them via `src/rovi_sim/config/bridge.yaml`:

- Color: `/camera/color/image_raw`
- Depth: `/camera/depth/image`

To make simulated optics “feel closer” to the real device, tweak the Gazebo sensor settings in the URDF (`horizontal_fov`, `width/height`, `near/far`).

## Native tools (optional, for lower-level sanity checks)

The robot setup uses Orbbec’s OpenNI2 runtime. One known-good install flow is:

```bash
sudo apt update
sudo apt install -y freeglut3-dev libglu1-mesa libgl1 unrar

mkdir -p ~/OpenNI && cd ~/OpenNI
unzip OpenNI_2.3.0.86_202210111155_4c8f5aa4_beta6_arm64.zip
unrar x 066797_OpenNI_2.3.0.86_202210111155_4c8f5aa4_beta6_a311d.rar

cd ~/OpenNI/OpenNI_2.3.0/rules
sudo chmod +x install.sh
sudo ./install.sh
sudo udevadm control --reload-rules
sudo udevadm trigger
```

Sanity checks:

- RGB snapshot (UVC):
  ```bash
  ls -la /dev/v4l/by-id/
  ffmpeg -hide_banner -loglevel error -y -f v4l2 \
    -i /dev/v4l/by-id/<your-camera>-video-index0 \
    -frames:v 1 rgb.png
  ```
- Depth (OpenNI2):
  ```bash
  cd ~/OpenNI/OpenNI_2.3.0/tools/NiViewer
  export OPENNI2_REDIST=$PWD
  export LD_LIBRARY_PATH=$PWD:$LD_LIBRARY_PATH
  ./SimpleRead
  ```

## Viewer-only build (PC)

If your PC is only a viewer and you don’t want to build the vendored `openni2_camera` overlay:
```bash
export ROVI_SKIP_OPENNI2=1
build
```
